{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7K4U8K0IaW-s"
   },
   "source": [
    "# Word Sense Disambiguation using Neural Networks\n",
    "Adam Ek\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Before starting, please read the instructions on [how to work on group assignments](https://github.com/sdobnik/computational-semantics/blob/master/README.md).\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NjDrL2PaW-u"
   },
   "source": [
    "A problem with static distributional vectors is the difficulty of distinguishing between different *word senses*. We will continue our exploration of word vectors by considering *trainable vectors* or *word embeddings* for Word Sense Disambiguation (WSD).\n",
    "\n",
    "The goal of word sense disambiguation is to train a model to find the sense of a word (homonyms of a word-form). For example, the word \"bank\" can mean \"sloping land\" or \"financial institution\". \n",
    "\n",
    "(a) \"I deposited my money in the **bank**\" (financial institution)\n",
    "\n",
    "(b) \"I swam from the river **bank**\" (sloping land)\n",
    "\n",
    "In case a) and b) we can determine that the meaning of \"bank\" based on the *context*. To utilize context in a semantic model we use *contextualized word representations*. Previously we worked with *static word representations*, i.e. the representation does not depend on the context. To illustrate we can consider sentences (a) and (b), the word **bank** would have the same static representation in both sentences, which means that it becomes difficult for us to predict its sense. What we want is to create representations that depend on the context, i.e. *contextualized embeddings*. \n",
    "\n",
    "We will create contextualized embeddings with Recurrent Neural Networks. You can read more about recurrent neural netoworks [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Your overall task in this lab is to create a neural network model that can disambiguate the word sense of 30 different words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mOFnOp3laW-v"
   },
   "outputs": [],
   "source": [
    "# first we import some packages that we need\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "# our hyperparameters (add more when/if you need them)\n",
    "device = torch.device('cuda:0')\n",
    "#device = torch.device('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9t7hP9DIaW-w"
   },
   "source": [
    "# 1. Working with data\n",
    "\n",
    "A central part of any machine learning system is the data we're working with. In this section we will split the data (the dataset is located here: ``wsd-data/wsd_data.txt``) into a training set and a test set. We will also create a baseline to compare our model against. Finally, we will use TorchText to transform our data (raw text) into a convenient format that our neural network can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrTqYvBYaW-x"
   },
   "source": [
    "## Data\n",
    "\n",
    "The dataset we will use contain different word sense for 30 different words. The data is organized as follows (values separated by tabs): \n",
    "- Column 1: word-sense\n",
    "- Column 2: word-form\n",
    "- Column 3: index of word\n",
    "- Column 4: white-space tokenized context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ9nM0HeaW-x"
   },
   "source": [
    "### Splitting the data\n",
    "\n",
    "Your first task is to seperate the data into a *training set* and a *test set*. The training set should contain 80% of the examples and the test set the remaining 20%. The examples for the test/training set should be selected **randomly**. Save each dataset into a .csv file for loading later. **[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QYjBmTjGaW-x"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def data_split(path_to_dataset):\n",
    "\n",
    "    with open (path_to_dataset, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    random.shuffle(content)                   # shuffle sentences to select randomly for train/test set\n",
    "    samples = len(content)\n",
    "    train = (samples*80)//100\n",
    "    train_set = content[:train]\n",
    "    test_set = content[train:]\n",
    "\n",
    "\n",
    "    with open('train.csv', 'w') as output:\n",
    "        for data in train_set:\n",
    "            output.write(data)\n",
    "\n",
    "    with open('test.csv', 'w') as output:\n",
    "        for data in test_set:\n",
    "            output.write(data)\n",
    "\n",
    "path = \"./wsd-data/wsd_data.txt\"\n",
    "data_split(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "knqjcHfmaW-y"
   },
   "source": [
    "### Creating a baseline\n",
    "\n",
    "Your second task is to create a *baseline* for the task. A baseline is a \"reality check\" for a model, given a very simple heuristic/algorithmic/model solution to the problem, can our neural network perform better than this?\n",
    "The baseline you are to create is the \"most common sense\" (MCS) baseline. For each word form, find the most commonly assigned sense to the word, and label a words with that sense. **[2 marks]**\n",
    "\n",
    "E.g. In a fictional dataset, \"bank\" have two senses, \"financial institution\" which occur 5 times and \"side of river\" 3 times. Thus, all 8 occurences of bank is labeled \"financial institution\" and this yields an MCS accuracy of 5/8 = 62.5%. If a model obtain a higher score than this, we can conclude that the model *at least* is better than selecting the most frequent word sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-W3riXWaW-y",
    "outputId": "0e15679a-2fb7-43b2-9801-eefb9c5f6fc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tAccuracy\t Number of senses\n",
      "keep.v   \t 39 %  \t\t 11\n",
      "national.a   \t 20 %  \t\t 6\n",
      "build.v   \t 21 %  \t\t 10\n",
      "place.n   \t 24 %  \t\t 7\n",
      "position.n   \t 20 %  \t\t 6\n",
      "serve.v   \t 16 %  \t\t 9\n",
      "hold.v   \t 15 %  \t\t 11\n",
      "line.n   \t 85 %  \t\t 11\n",
      "see.v   \t 63 %  \t\t 11\n",
      "time.n   \t 28 %  \t\t 5\n",
      "physical.a   \t 24 %  \t\t 6\n",
      "follow.v   \t 15 %  \t\t 11\n",
      "regular.a   \t 22 %  \t\t 8\n",
      "bad.a   \t 61 %  \t\t 4\n",
      "force.n   \t 16 %  \t\t 8\n",
      "professional.a   \t 22 %  \t\t 5\n",
      "security.n   \t 20 %  \t\t 7\n",
      "positive.a   \t 35 %  \t\t 5\n",
      "point.n   \t 36 %  \t\t 8\n",
      "common.a   \t 25 %  \t\t 4\n",
      "find.v   \t 23 %  \t\t 10\n",
      "life.n   \t 22 %  \t\t 9\n",
      "order.n   \t 22 %  \t\t 5\n",
      "bring.v   \t 21 %  \t\t 8\n",
      "active.a   \t 32 %  \t\t 5\n",
      "extend.v   \t 18 %  \t\t 7\n",
      "case.n   \t 20 %  \t\t 8\n",
      "lead.v   \t 18 %  \t\t 8\n",
      "critical.a   \t 27 %  \t\t 5\n",
      "major.a   \t 30 %  \t\t 4\n",
      "\n",
      "Baseline accuracy:  28%\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from collections import Counter\n",
    "\n",
    "def mcs_baseline(data):\n",
    "    form_sense = {}\n",
    "    mcs = {}\n",
    "    for_accuracy = {}\n",
    "\n",
    "    with open (data, \"r\") as f:\n",
    "        content = f.readlines()\n",
    "    for data in content:\n",
    "        new = data.strip().split('\\t')\n",
    "        if new[1] not in form_sense:\n",
    "            form_sense[(new[1])] = []\n",
    "        form_sense[(new[1])].append(new[0])\n",
    "    for key, value in form_sense.items():\n",
    "        counter=collections.Counter(value)\n",
    "        counter = dict(Counter(counter))\n",
    "        form_sense[key] = counter\n",
    "    for key1, dic in form_sense.items(): # loop again due to run time error\n",
    "        total = sum(dic.values()) # sum the number of senses for each lemma\n",
    "        sorted_dic = {k: v for k, v in sorted(dic.items(), key=lambda item: item[1])} # sort dictionary\n",
    "        sort_values = [sorted_dic[x] for x in list(sorted_dic)] # get sorted values as list\n",
    "        mcs[key1] = [float(\"{:.2}\".format(sort_values[-1]/total)),len(sort_values)]\n",
    "        for_accuracy[(list(dic)[-1])] = float(\"{:.2}\".format(sort_values[-1]/total))\n",
    "        #key of the for_accuracy dictionary is the most frequent sense for a specific lemma\n",
    "        #value is the mcs accuracy for the corresponding lemma\n",
    "\n",
    "    total_accuracy = sum(list(for_accuracy.values()))/len(mcs)\n",
    "    print(\"\\t\\tAccuracy\\t Number of senses\")\n",
    "    [print(key,'  \\t',int(value[0]*100),\"%  \\t\\t\",value[1]) for key, value in mcs.items()]\n",
    "    print(\"\\nBaseline accuracy: \", str(int(total_accuracy*100))+\"%\")\n",
    "\n",
    "\n",
    "path = \"./wsd-data/wsd_data.txt\"\n",
    "mcs_baseline(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FI3NH72xaW-z"
   },
   "source": [
    "### Creating data iterators\n",
    "\n",
    "To train a neural network, we first need to prepare the data. This involves converting words (and labels) to a number, and organizing the data into batches. We also want the ability to shuffle the examples such that they appear in a random order.  \n",
    "\n",
    "To do all of this we will use the torchtext library (https://torchtext.readthedocs.io/en/latest/index.html). In addition to converting our data into numerical form and creating batches, it will generate a word and label vocabulary, and data iterators than can sort and shuffle the examples. \n",
    "\n",
    "Your task is to create a dataloader for the training and test set you created previously. So, how do we go about doing this?\n",
    "\n",
    "1) First we create a ``Field`` for each of our columns. A field is a function which tokenize the input, keep a dictionary of word-to-numbers, and fix paddings. So, we need four fields, one for the word-sense, one for the position, one for the lemma and one for the context. \n",
    "\n",
    "2) After we have our fields, we need to process the data. For this we use the ``TabularDataset`` class. We pass the name and path of the training and test files we created previously, then we assign which field to use in each column. The result is that each column will be processed by the field indicated. So, the context column will be tokenized and processed by the context field and so on. \n",
    "\n",
    "3) After we have processed the dataset we need to build the vocabulary, for this we call the function ``build_vocab()`` on the different ``Fields`` with the output from ``TabularDataset`` as input. This looks at our dataset and creates the necessary vocabularies (word-to-number mappings). \n",
    "\n",
    "4) Finally, the last step. In the last step we load the data objects given by the ``TabularDataset`` and pass it to the ``BucketIterator`` class. This class will organize our examples into batches and shuffle them around (such that for each epoch the model observe the examples in a different order). When we are done with this we can let our function return the data iterators and vocabularies, then we are ready to train and test our model!\n",
    "\n",
    "Implement the dataloader. [**2 marks**]\n",
    "\n",
    "*hint: for TabularDataset and BucketIterator use the class function splits()* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2WLfuEt8aW-0",
    "outputId": "9952987b-e87e-4661-efed-ac77af3b36a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchtext.legacy.data.iterator.BucketIterator at 0x7f53fc2fa410>,\n",
       " <torchtext.legacy.data.iterator.BucketIterator at 0x7f53fa512110>,\n",
       " <torchtext.legacy.data.field.Field at 0x7f541b50c510>,\n",
       " <torchtext.legacy.data.field.Field at 0x7f541b50c590>,\n",
       " <torchtext.legacy.data.field.Field at 0x7f541b50c5d0>,\n",
       " <torchtext.legacy.data.field.Field at 0x7f541b50c610>)"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def dataloader():\n",
    "    ddir = './'\n",
    "    whitespacer = lambda x: x.split(' ')\n",
    "\n",
    "    # \"fields\" that process the different columns in our CSV files\n",
    "    # similar to assignment 3 given example\n",
    "    \n",
    "    WORD_SENSE = Field(tokenize    = whitespacer,\n",
    "                       batch_first = True)\n",
    "    \n",
    "    POSITION = Field(batch_first = True,\n",
    "                     sequential = False,\n",
    "                     use_vocab = False)\n",
    "    \n",
    "    LABELS = Field(tokenize    = whitespacer,\n",
    "                   batch_first = True) # enforce the (batch, words) structure\n",
    "    \n",
    "    CONTEXT = Field(tokenize    = whitespacer,\n",
    "                    lower       = True,\n",
    "                    batch_first = True)\n",
    "\n",
    "    # read the csv files\n",
    "    train, test = TabularDataset.splits(path   = ddir,\n",
    "                                        train  = 'train.csv',\n",
    "                                        test   = 'test.csv',\n",
    "                                        format = 'csv',\n",
    "                                        fields = [('word_sense', WORD_SENSE),\n",
    "                                                  ('lemma', LABELS),\n",
    "                                                  ('position', POSITION),\n",
    "                                                  ('context', CONTEXT)],\n",
    "                                        skip_header = False,\n",
    "                                        csv_reader_params = {'delimiter':'\\t',\n",
    "                                                             'quotechar':'½'})\n",
    "\n",
    "    # build vocabularies based on what our csv files contained and create word2id mapping\n",
    "    CONTEXT.build_vocab(train)\n",
    "    LABELS.build_vocab(train)\n",
    "    WORD_SENSE.build_vocab(train)\n",
    "\n",
    "    # create batches from our data, and shuffle them for each epoch\n",
    "    train_iter, test_iter = BucketIterator.splits((train, test),\n",
    "                                                  batch_size        = 8,\n",
    "                                                  sort_within_batch = True,\n",
    "                                                  sort_key          = lambda x: len(x.context),\n",
    "                                                  shuffle           = True,\n",
    "                                                  device            = device)\n",
    "\n",
    "    return train_iter, test_iter, WORD_SENSE, POSITION, LABELS, CONTEXT \n",
    "dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-TWkGSmaW-1"
   },
   "source": [
    "# 2.1 Creating and running a Neural Network for WSD\n",
    "\n",
    "In this section we will create and run a neural network to predict word senses based on *contextualized representations*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wl5KE3L3aW-1"
   },
   "source": [
    "### Model\n",
    "\n",
    "We will use a bidirectional Long-Short-Term Memory (LSTM) network to create a representation for the sentences and a Linear classifier to predict the sense of each word.\n",
    "\n",
    "When we initialize the model, we need a few things:\n",
    "\n",
    "    1) An embedding layer: a dictionary from which we can obtain word embeddings\n",
    "    2) A LSTM-module to obtain contextual representations\n",
    "    3) A classifier that compute scores for each word-sense given *some* input\n",
    "\n",
    "\n",
    "The general procedure is the following:\n",
    "\n",
    "    1) For each word in the sentence, obtain word embeddings\n",
    "    2) Run the embedded sentences through the RNN\n",
    "    3) Select the appropriate hidden state\n",
    "    4) Predict the word-sense \n",
    "\n",
    "**Suggestion for efficiency:**  *Use a low dimensionality (32) for word embeddings and the LSTM when developing and testing the code, then scale up when running the full training/tests*\n",
    "    \n",
    "Your tasks will be to create two different models (both follow the two outlines described above), described below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9MBfiRhaW-1"
   },
   "source": [
    "In the first approach to WSD, you are to select the index of our target word (column 3 in the dataset) and predict the word sense. **[5 marks]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "R8MLuzXCaW-2"
   },
   "outputs": [],
   "source": [
    "# documentation consulted https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "\n",
    "class WSDModel_approach1(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocabulary_dim, sense_num): \n",
    "        super(WSDModel_approach1, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocabulary_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first = True, bidirectional=True)\n",
    "        self.classifier = nn.Linear(hidden_dim*2, sense_num) \n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # embedd the words in out sentence\n",
    "        embedded_batch = self.embeddings(batch.context)\n",
    "        # create contextualized representations for our words\n",
    "\n",
    "        contextualized_representations, _ = self.rnn(embedded_batch)\n",
    "        # 1) output features of the hidden state from all time steps\n",
    "        # 2) hidden state for the step 'n' which we do not need in this case \n",
    "        \n",
    "        out = contextualized_representations[range(contextualized_representations.shape[0]),batch.position]\n",
    "        predictions = self.classifier(out) \n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDVt3s8JaW-2"
   },
   "source": [
    "In the second approach to WSD, you are to predict the word sense based on the final hidden state given by the RNN. **[5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6LyEbxc5aW-2"
   },
   "outputs": [],
   "source": [
    "class WSDModel_approach2(nn.Module):\n",
    "    def __init__(self,embedding_dim, hidden_dim, vocabulary_dim, sense_num):\n",
    "        super(WSDModel_approach2, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocabulary_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first = True, bidirectional = True)\n",
    "        self.classifier = nn.Linear(hidden_dim*2, sense_num) \n",
    "        \n",
    "    \n",
    "    def forward(self, batch):\n",
    "        # embedd the words in out sentence\n",
    "        embedded_batch = self.embeddings(batch.context)\n",
    "        \n",
    "        contextualized_representations, (h0, c0) = self.rnn(embedded_batch)\n",
    "        #the new shape of our output is: batch size, sequence length, hidden size\n",
    "        # decode the hidden state only for the last time step\n",
    "\n",
    "        forward = h0[0,:,:]\n",
    "        backward = h0[1,:,:]\n",
    "        conc = torch.cat((forward,backward),1)\n",
    "\n",
    "        # h_n: tensor with hidden state\n",
    "        # c_n: tensor with cell state\n",
    "\n",
    "        predictions = self.classifier(conc)\n",
    "        \n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrdi4TD1aW-2"
   },
   "source": [
    "### Training and testing the model\n",
    "\n",
    "Now we are ready to train and test our model. What we need now is a loss function, an optimizer, and our data. \n",
    "\n",
    "- First, create the loss function and the optimizer.\n",
    "- Next, we iterate over the number of epochs (i.e. how many times we let the model see our data). \n",
    "- For each epoch, iterate over the dataset (``train_iter``) to obtain batches. Use the batch as input to the model, and let the model output scores for the different word senses.\n",
    "- For each model output, calculate the loss (and print the loss) on the output and update the model parameters.\n",
    "- Reset the gradients and repeat.\n",
    "- After all epochs are done, test your trained model on the test set (``test_iter``) and calculate the total and per-word-form accuracy of your model.\n",
    "\n",
    "Implement the training and testing of the model **[4 marks]**\n",
    "\n",
    "**Suggestion for efficiency:** *when developing your model, try training and testing the model on one or two batches (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ADZZ4E1vaW-3"
   },
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs :)\n",
    "hyperparameters = {'epochs':3,\n",
    "                   'batch_size':8,\n",
    "                   'learning_rate':0.001,\n",
    "                   'embedding_dim':32,\n",
    "                   'hidden_dim':32}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "LPLtmzUIaW-3"
   },
   "outputs": [],
   "source": [
    "# documentation consulted https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "\n",
    "train_iter, test_iter, sense, position, labels, vocab = dataloader()\n",
    "\n",
    "WSDModel1 = WSDModel_approach1(hyperparameters['embedding_dim'], \n",
    "                               hyperparameters['hidden_dim'],\n",
    "                               len(vocab.vocab),\n",
    "                               len(sense.vocab))\n",
    "\n",
    "WSDModel2 = WSDModel_approach2(hyperparameters['embedding_dim'], \n",
    "                               hyperparameters['hidden_dim'],\n",
    "                               len(vocab.vocab),\n",
    "                               len(sense.vocab))\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss() # the only Loss function used so far, so I follow the map (also, best for classification problems?)\n",
    "WSDModel1.to(device)\n",
    "WSDModel2.to(device)\n",
    "optimizer1 = optim.Adam(WSDModel1.parameters(), hyperparameters ['learning_rate'])\n",
    "optimizer2 = optim.Adam(WSDModel2.parameters(), hyperparameters ['learning_rate'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xO3JsLdaW-3",
    "outputId": "c9707699-7ee5-47ed-ca63-f5e818b75c9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 60840 1.5809\n",
      "1 60840 2.5694\n",
      "2 60840 3.4174\n",
      "0 60840 5.4067\n",
      "1 60840 10.8134\n",
      "2 60840 16.22\n"
     ]
    }
   ],
   "source": [
    "#TRAIN MODEL \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model,name):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for epoch in range(hyperparameters['epochs']):\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            \n",
    "            senses    = batch.word_sense\n",
    "\n",
    "            # run sentences through the model\n",
    "            output = model(batch) #forward parameter: batch\n",
    "\n",
    "\n",
    "            batch_loss = loss_function(output, senses.reshape(-1))\n",
    "            total_loss += batch_loss.item()\n",
    "            \n",
    "            # report results\n",
    "            print(epoch, (i+1)*senses.size(0), np.round(total_loss/(i+1),4),end='\\r')\n",
    "\n",
    "            # calculate gradients\n",
    "            batch_loss.backward()\n",
    "            # update model weights\n",
    "            optimizer1.step()\n",
    "            # reset gradients\n",
    "            optimizer1.zero_grad()\n",
    "        print()\n",
    "    torch.save(model, \"./\"+name+ \".pt\")       \n",
    "    \n",
    "\n",
    "train_model(WSDModel1,\"WSDModel1\")\n",
    "train_model(WSDModel2,\"WSDModel2\")\n",
    "# test model after all epochs are completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey_TWkNFuIAC",
    "outputId": "765c9fbc-feb7-426a-97b1-125368a102f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WSDModel1 accuracy: 0.67\n",
      "WSDModel2 accuracy: 0.01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# TEST MODEL\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    classes = torch.argmax(predictions, dim=1)\n",
    "    return torch.mean((classes == labels).float())\n",
    "    \n",
    "\n",
    "def test_model(model,name):\n",
    "    test_loss = 0\n",
    "    model.eval()\n",
    "    running_accuracy = 0.00\\\n",
    "    num_senses = len(sense.vocab)\n",
    "    \n",
    "    for i, batch in enumerate(test_iter):\n",
    "        senses    = batch.word_sense\n",
    "        classes   = batch.lemma\n",
    "        \n",
    "        with torch.no_grad(): # dont collect gradients when testing\n",
    "            output = model(batch)\n",
    "        batch_loss = loss_function(output, senses.reshape(-1))\n",
    "        test_loss += batch_loss.item()\n",
    "        running_accuracy += accuracy(output, senses.reshape(-1))\n",
    "\n",
    "    \n",
    "    running_accuracy /= len(test_iter) # a tensor that contains accuracy and device\n",
    "    print(name + \" accuracy:\",np.round(running_accuracy.item(),2)) # print accuracy only\n",
    "\n",
    "WSDModel1 = torch.load(\"./WSDModel1.pt\")\n",
    "WSDModel2 = torch.load(\"./WSDModel2.pt\")\n",
    "\n",
    "test_model(WSDModel1,\"WSDModel1\")\n",
    "test_model(WSDModel2,\"WSDModel2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is completely weird. The accuracy for model1 looks fine, but we have no idea why we get this result for model2, although everything seem to be correct.\n",
    "### Also, we tried multiple ways but did not manage to compute the accuracy for each label :/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etZzRHlFaW-4"
   },
   "source": [
    "# 2.2 Running a transformer for WSD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUvHwGo5aW-4"
   },
   "source": [
    "In this section of the lab you'll try out the transformer, specifically the BERT model. For this we'll use the huggingface library (https://huggingface.co/).\n",
    "\n",
    "You can find the documentation for the BERT model here (https://huggingface.co/transformers/model_doc/bert.html) and a general usage guide here (https://huggingface.co/transformers/quickstart.html).\n",
    "\n",
    "What we're going to do is *fine-tune* the BERT model, i.e. update the weights of a pre-trained model. That is, we have a model that is trained on language modeling, but now we apply it to word sense disambiguation with the word representations it learnt from language modeling.\n",
    "\n",
    "We'll use the same data splits for training and testing as before, but this time you'll not use a torchtext dataloader. Rather now you create an iterator that collects N sentences (where N is the batch size) then use the BertTokenizer to transform the sentence into integers. For your dataloader, remember to:\n",
    "* Shuffle the data in each batch\n",
    "* Make sure you get a new iterator for each *epoch*\n",
    "* Create a vocabulary of *sense-labels* so you can calculate accuracy \n",
    "\n",
    "We then pass this batch into the BERT model and train as before. The BERT model will encode the sentence, then we send this encoded sentence into a prediction layer (you can either the the sentence-representation from bert, or the ambiguous word) like before and collect sense predictions.\n",
    "\n",
    "About the hyperparameters and training:\n",
    "* For BERT, usually a lower learning rate works best, between 0.0001-0.000001.\n",
    "* BERT takes alot of resources, running it on CPU will take ages, utilize the GPUs :)\n",
    "* Since BERT takes alot of resources, use a small batch size (4-8)\n",
    "* Computing the BERT representation, make sure you pass the mask\n",
    "\n",
    "**[10 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7bHTTUzwaW-4"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "def dataloader_for_bert(path_to_file, batch_size):\n",
    "    #numerical representations of tokens building the sequences that will be used as input by the model\n",
    "    token_indices = [] \n",
    "    #The attention mask is an optional argument used when batching sequences together. This argument indicates to the model which tokens should be attended to, and which should not.\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    with open (path_to_file, \"r\") as f:\n",
    "        content = f.readlines()    \n",
    "\n",
    "    random.shuffle(content)\n",
    "\n",
    "    for i in range((len(content)//batch_size)+1):\n",
    "        if len(content) < batch_size:\n",
    "            batches = content\n",
    "            del content[:]\n",
    "        else:\n",
    "            batches = content[:batch_size]\n",
    "            del content[:batch_size]\n",
    "        for batch in batches:\n",
    "            context = batch.split(\"\\t\")[3]\n",
    "            sense = batch.split(\"\\t\")[0]\n",
    "            labels.append(sense)\n",
    "            tokenized = tokenizer.batch_encode_plus(context, padding=True, return_attention_mask=True).to(device)\n",
    "            #The tokenizer returns a dictionary with all the arguments necessary for its corresponding model to work properly. The token indices are under the key “input_ids”:\n",
    "            token_indices.append(tokenized['input_ids'])\n",
    "            attention_masks.append(tokenized['attention_mask'])\n",
    "            \n",
    "    return labels, token_indices, attention_masks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 766
    },
    "id": "kqrljCRDQpd8",
    "outputId": "d8c75fa8-149e-4a8c-dbba-02510549d821",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2233a0a2a6fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabels2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_indices2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader_for_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/04-word-sense-diasambiguation/test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_indices1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataloader_for_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/04-word-sense-diasambiguation/train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_indices2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_masks2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken_indices1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_masks1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-08e00db51b33>\u001b[0m in \u001b[0;36mdataloader_for_bert\u001b[0;34m(path_to_file, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0msense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0;31m#The tokenizer returns a dictionary with all the arguments necessary for its corresponding model to work properly. The token indices are under the key “input_ids”:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mtoken_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1582\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Method `{func.__name__}` requires PyTorch.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;31m# into a HalfTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;31m# into a HalfTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_torch_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempting to cast a BatchEncoding to type {str(device)}. This is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "labels2, token_indices2, attention_masks2 = dataloader_for_bert(\"/content/drive/MyDrive/04-word-sense-diasambiguation/test.csv\",4)\n",
    "labels1, token_indices1, attention_masks1 = dataloader_for_bert(\"/content/drive/MyDrive/04-word-sense-diasambiguation/train.csv\",4)\n",
    "test_iter = zip(labels2,token_indices2,attention_masks2)\n",
    "train_iter = zip(labels1,token_indices1,attention_masks1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "xX21Lg5IaW-4"
   },
   "outputs": [],
   "source": [
    "class BERT_WSD(nn.Module):\n",
    "    def __init__(self, in_put):\n",
    "        super(BERT_WSD, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, in_put)\n",
    "    \n",
    "    def forward(self, batch): # batch contains input_ids, masks and sense labels\n",
    "        try:\n",
    "            out = self.bert(batch.token_indices1,batch.attention_masks1)\n",
    "        except:\n",
    "            out = self.bert(batch.token_indices2,batch.attention_masks2)\n",
    "        hidden_repr = out[:,0,:]\n",
    "        predictions = self.classifier(hidden_repr)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "JSLcLD4TaW-5",
    "outputId": "cd2737ee-4da6-46fe-fd41-662b7fd06bdb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-c4cf4848a062>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_indices1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_masks1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'token_indices1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-9277c0439eac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparameters\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Converting to cuda tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-c4cf4848a062>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_indices1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_masks1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_indices2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_masks2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mhidden_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_repr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'token_indices2'"
     ]
    }
   ],
   "source": [
    "# Documentation\n",
    "# https://mccormickml.com/2019/07/22/BERT-fine-tuning/\n",
    "# https://medium.com/analytics-vidhya/a-gentle-introduction-to-implementing-bert-using-hugging-face-35eb480cff3\n",
    "\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "model = BERT_WSD(len(labels.vocab))\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), hyperparameters ['learning_rate'])\n",
    "\n",
    "# Not sure if we need the scheduler, but we found it in 2 tutorials\n",
    "total_steps = hyperparameters ['batch_size'] * hyperparameters ['epochs']\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 1000, \n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# TRAINING\n",
    "total_loss = 0\n",
    "model.train()\n",
    "for _ in range(hyperparameters ['epochs']):\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        output = model(batch)\n",
    "\n",
    "    # Converting to cuda tensors\n",
    "        batch_var = (t.to(device) for t in batch)\n",
    "        labels, indices, attn_masks = batch_var\n",
    "\n",
    "        batch_loss = loss(output, labels)\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "    print()\n",
    "\n",
    "# TEST   \n",
    "def accuracy(predictions, labels):\n",
    "    classes = torch.argmax(predictions, dim=1)\n",
    "    return torch.mean((classes == labels).float())    \n",
    "# test model after all epochs are completed\n",
    "test_loss = 0\n",
    "running_accuracy = 0.00\n",
    "model.eval()\n",
    "for i, batch in enumerate(test_iter):\n",
    "    batch_var = (t.to(device) for t in batch)\n",
    "    labels, indices, attn_masks = batch_var\n",
    "    with torch.no_grad():\n",
    "        output = model(batch)\n",
    "    batch_loss = loss(output, labels)\n",
    "    test_loss += batch_loss.item()\n",
    "    running_accuracy += accuracy(output, labels)\n",
    "running_accuracy /= len(test_iter) # a tensor that contains accuracy and device\n",
    "print(\"Model accuracy:\",np.round(running_accuracy.item(),2)) # print accuracy only\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1vcz8K0aW-5"
   },
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT8J0bIUaW-5"
   },
   "source": [
    "Explain the difference between the first and second approach. What kind of representations are the different approaches using to predict word-senses? **[4 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYVlQdjQaW-5"
   },
   "source": [
    "In the first approach we try to predict the sense of a specific word based on the representation of the target word (using the index). In the second approach we consider only the final hidden state, which we believe it means that all the words(context/sentence) are used to predict the sense of a word (since we provide no specific index to find the sense of the target word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9EL96W4aW-5"
   },
   "source": [
    "Evaluate your model with per-word-form *accuracy* and comment on the results you get, how does the model perform in comparison to the baseline, and how do the models compare to each other? \n",
    "\n",
    "Expand on the evaluation by sorting the word-forms by the number of senses they have. Are word-forms with fewer senses easier to predict? Give a short explanation of the results you get based on the number of senses per word.\n",
    "\n",
    "**[6 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we did not manage to calculate the per-word-form accuracy and also train the BERT model. We will attempt though to provide some explanations according to our intuition.\n",
    "\n",
    "We suspect that Model1 with 67% accuracy would be definetely better than the baseline, but we cannot be sure concerning the performance of Model2. \n",
    "\n",
    "We would also suspect that maybe words with fewer senses are easier to predict because there are less chances for our model to be wrong when trying to predict between less classes. At the same time though this is a matter of probability so it is not a stable conclusion. Thus, depending on our data, eventually the number of senses for each lemma does not matter after all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEAX6NUDaW-5"
   },
   "source": [
    "How does the LSTMs perform in comparison to BERT? What's the difference between representations obtained by the LSTMs and BERT? **[2 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYdGGN0laW-5"
   },
   "source": [
    "We are pretty confident that BERT should present the best results out of all our models, especially since pre-trained embeddings are applied. BERT takes into consideration the meaning of the whole sentence (at least this is what we tried to do in the forward function), so it could be compared to Model's2 approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaZwqlZ5aW-6"
   },
   "source": [
    "What could we do to improve our LSTM word sense disambiguation models and our BERT model? **[4 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoHGD7v2aW-6"
   },
   "source": [
    "The very first suggestion we always propose to improve our models' perfomances is to use more data. But as we know, more data is not always the best approach, if data is not selected wisely. For instance, if we add more data but concern only specidic senses, then it won't make a great difference. A wider variety in the senses category is needed for our models to perform better. Also, we should test different epochs, number of batches, embedding dimensionality, hidden layers and learning rate.\n",
    "\n",
    "For BERT, a \"clever\" optimization can lead to improvement (learning rate,number of warmup steps, training steps, larger batch size). Batch size was suggested to be small, so a greater size could bring better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eFS6AQOaW-6"
   },
   "source": [
    "# Readings:\n",
    "\n",
    "[1] Kågebäck, M., & Salomonsson, H. (2016). Word Sense Disambiguation using a Bidirectional LSTM. arXiv preprint arXiv:1606.03568.\n",
    "\n",
    "[2] https://cl.lingfil.uu.se/~nivre/master/NLP-LexSem.pdf"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Αντίγραφο 04-word-sense-disambiguation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
